{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYdG4fWykqQ+fPPwVbWu/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI_4170_summer/blob/main/Lecture_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 15\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. Large Language Models (LLMs)\n",
        "2. Components of LLMs\n",
        "3. Capabilities and Applications\n",
        "4. Limitations\n",
        "5. BERT VS GPT"
      ],
      "metadata": {
        "id": "ytQZ0r_gSnRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "An LLM is a probabilistic model that assigns probability values to sequences of words or tokens in a given language. The goal is to capture the underlying pattern in the data to predict the likelihood of a sequence of words.\n",
        "The LLms of today are advanced models trained on massive amounts of data and are designed to generate human like text.\n"
      ],
      "metadata": {
        "id": "vnD6eDSPS5_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components of LLMs\n",
        "\n",
        "Most of the recent LLMs are built on the Transformer architecture. At the heart of the Transformer are two fundamental principles: the use of self-attention mechanisms and an encoder-decoder structure.\n",
        "\n",
        "For more details refer to Lecture 14 material."
      ],
      "metadata": {
        "id": "kz8yogX6V-Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of LLMs\n",
        "\n",
        "1. Language Representation Models: These models have a bidirectional context understanding. These are versatile because they can be used for various downstream tasks\n",
        "\n",
        "[Reading](https://www.mdpi.com/1099-4300/23/11/1422#:~:text=The%20basis%20for%20each%20neural,are%20usually%20called%20word%20embeddings.)\n",
        "\n",
        "\n",
        "2. Zero shot learning models: Zero-shot learning (ZSL) is a machine learning technique that allows pre-trained models to predict class labels for data samples that aren't present in the training data. ZSL methods work by using auxiliary information to associate observed and non-observed classes. This information encodes observable distinguishing properties of objects. The GPT series of language models ahve this generative capability as these models have been trained on diverse datasets.\n",
        "\n",
        "3. Multi-shot learning models: These models excel in adapting to tasks with few examples. The essence of multi-shot learning lies in providing the model with limited examples for a specific task, allowing it to perform well with minimal training.\n",
        "\n",
        "[Reading](https://www.analyticsvidhya.com/blog/2022/12/know-about-zero-shot-one-shot-and-few-shot-learning/)\n",
        "\n",
        "4. Fine Tuned or domain specific models: These reperesent a category of models that undergo additional training tailored to a specific task or domain. Fine-tuning enables these models to adapt to the intricacies of specific tasks or domains, resulting in improved performance and task-specific expertise.\n",
        "\n",
        "5. Retrieval Augemnted generation (RAG): RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs). [Source](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en)"
      ],
      "metadata": {
        "id": "vZR2VLfsYSYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters in some of the recent LLMs:\n",
        "\n",
        "1. GPT4: 1.7 trillion parameters\n",
        "2. Llama: 65 billion\n",
        "3. BARD: 137 billion parameters"
      ],
      "metadata": {
        "id": "PA7SwYJCdWjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Capabilities\n",
        "\n",
        "1. Language Modeling:A language model is a type of machine learning model trained to conduct a probability distribution over words. Put it simply, a model tries to predict the next most appropriate word to fill in a blank space in a sentence or phrase, based on the context of the given text.\n",
        "[Link](https://www.altexsoft.com/blog/language-models-gpt/)\n",
        "Evaluation Metric: Perplexity\n",
        "Note: The test perplexity value of GPT-3 surpasses all other language models created before July 2020 with a score of 20.5. Also note that in most of the other tasks, GPT-3 beats other SOTA models.\n",
        "2. Question Answering: Question answering involves providing a model with a question and a context (such as a passage of text), and the model generates an accurate and relevant answer based on its understanding of the content.\n",
        "Accuracy is the metric to measure how well the model answered all the questions.\n",
        "3. Translations: Translation involves converting text from one language to another while preserving the meaning.\n",
        "Bilingual Evaluation Understudy (BLEU) measures the quality of machine-translated text by comparing it to one or more human reference translations.\n",
        "ROUGE is a set of metrics that compare machine-generated texts to a set of references (typically human-generated). The main focus of ROUGE is to measure the overlap of n-grams, word sequences, and word pairs between the machine-generated and the reference texts.\n",
        "4. Arithmetic Problem Solving: Arithmetic tasks involve performing mathematical operations using natural language input, such as addition, subtraction, multiplication, or division. The model understands the numerical and symbolic representations and provides the correct result.\n",
        "[Paper](https://arxiv.org/pdf/2401.01312.pdf)\n",
        "5. Text Generation: Generating related to a specific topic or area. BLEU score is the most common evaluation metric.\n",
        "\n",
        "[Link](https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1)"
      ],
      "metadata": {
        "id": "FuSRN7WTeUcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning an LLM\n",
        "\n",
        "Fine tuning is the process of adjusting paramters of a pretrained LLM on a specific dataset to increase its performance on a specific task. For example the first version of GPT-3 was trained on a huge and diverse dataset for text completion.These models could accurately predict the next word in a sentence. However, as they were not trained to specifically follow instructions from users, these models had the potential of generating inaccurate or misleading outputs unrelated to the usersâ€™ instructions.\n",
        "\n",
        "The researchers at OpenAI fine-tuned GPT-3 on a prompt-based dataset to make the models safer and more helpful for their users. This fine-tuning resulted in the InstructGPT models that excel at following user prompts and generating outputs more aligned with the given instructions.\n",
        "\n",
        "In general fine-tuning requires an iterative training scheme such that the existing paramters of the model are updated as per the new datasets.\n",
        "Examples of fine tuned models: ChatGPT (Fine tuned for conversational text),\n",
        "Koala (chatbot fine tuned for academic research): [Link](https://bair.berkeley.edu/blog/2023/04/03/koala/) , Starcoder(model fine-tuned on a diverse and extensive code dataset from GitHub, which includes a variety of programming languages and related content such as Git commits, GitHub issues, and Jupyter notebooks), [Link](https://github.com/bigcode-project/starcoder)\n",
        "\n"
      ],
      "metadata": {
        "id": "6P6KXYMTjDWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n",
        "\n",
        "Criteria for model selection:\n",
        "\n",
        "1. Model Size: Model size impacts model performance but larger models require more computational power. Depending on our reuirements we may opt for a smaller model like GPT-2, that has 124 million parameters or choose a more powerful option like Llama-2, which has 70 billion parameters and provides a higher level of performance.\n",
        "\n",
        "2. Pretraining: A knowledge of the pretraining of the LLM greatly influences our understanding of user prompts and outputs. Any model trained on internet data will be diverse e.g. Common Crawl was used to train GPT-3, Llama. This will also give us a good understanding of the limitation of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "BA19T1PHirpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing LLMs\n",
        "\n",
        "1. GitHub\n",
        "\n",
        "2. Hugging Face: It includes an expansive repository for machine learning models and is particularly known for its collection of pretrained LLMs."
      ],
      "metadata": {
        "id": "cnuZ5_kKocds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations\n",
        "\n",
        "1. Bias and fairness: LLMs are susceptible to biases present in the data they are trained on, potentially perpetuating and even amplifying existing societal biases.\n",
        "\n",
        "2. Common Sense: LLMs might struggle with grasping reasoning based on common sense. Their responses are often generated based on statistical patterns in the training data, leading to occasional lapses in logical reasoning and understanding context.\n",
        "\n",
        "3. Over-reliance on Training Data: The model might produce inaccurate or skewed results if the training data is limited or biased.\n",
        "\n",
        "4. Ethical Concerns: Ethical considerations arise regarding the potential misuse of LLMs, including the generation of misinformation, deepfakes, or malicious content.\n",
        "\n",
        "5. lack of Explainability: LLMs often operate as black boxes, making it difficult to understand the reasoning behind their responses.\n",
        "\n",
        "6. Resource Intensive: Any application involves training of large models that reuire significan t computational resources.\n",
        "\n",
        "7. Contextual understanding: Mostly lack understanding of deep contextual matter."
      ],
      "metadata": {
        "id": "pNTboK-TgH0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Example Code](https://github.com/Uzmamushtaque/Projects-in-Machine-Learning-and-AI/blob/main/GenerateSongLyrics1.ipynb)"
      ],
      "metadata": {
        "id": "cUMMemgNfOkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Recent Research](https://paperswithcode.com/task/large-language-model)"
      ],
      "metadata": {
        "id": "kcUefFH_IuIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Papers\n",
        "[BERT](https://arxiv.org/pdf/1810.04805)\n",
        "\n",
        "BERT, introduced by Google in 2018, revolutionized NLP by enabling bidirectional context understanding in language models. The core idea is to pre-train deep bidirectional representations and fine-tune them for specific tasks with minimal architectural modifications. BERT uses the transformer encoder to learn context from both the left and right of a word, improving performance on language understanding tasks.\n",
        "\n",
        "**Pre-training:** Learn general language representations on large corpora (e.g., Wikipedia, BooksCorpus).\n",
        "\n",
        "**Fine-tuning:** Adapt the model to downstream tasks (e.g., sentiment analysis, Q&A) by adding simple output layers.\n",
        "\n",
        "BERT is trained on the following two objectives:\n",
        "\n",
        "Masked Language Modeling (MLM):\n",
        "Randomly masks 15% of input tokens and trains the model to predict them.\n",
        "Enables bidirectional context learning, unlike traditional left-to-right models.\n",
        "\n",
        "Next Sentence Prediction (NSP):\n",
        "Trains the model to understand relationships between sentences.\n",
        "Given a pair of sentences, the model predicts if the second sentence follows the first in the original text.\n",
        "Useful for tasks like Question Answering and Natural Language Inference.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "Built solely on the Transformer encoder (no decoder).\n",
        "\n",
        "Available in two main sizes:\n",
        "\n",
        "BERT-Base: 12 layers (transformer blocks), 768 hidden units, 12 attention heads (110M parameters).\n",
        "\n",
        "BERT-Large: 24 layers, 1024 hidden units, 16 attention heads (340M parameters).\n",
        "\n",
        "Innovations:\n",
        "\n",
        "Achieved state-of-the-art results on several NLP benchmarks (e.g., GLUE, SQuAD, SWAG).\n",
        "Outperformed previous models by understanding context more deeply.\n",
        "Inspired a wave of successor models like RoBERTa, ALBERT, and DistilBERT that built on its architecture and training insights.\n",
        "\n",
        "Limitations:\n",
        "Expensive to train: Large computational resources needed.\n",
        "\n",
        "Slow inference: Due to its large size and bidirectional processing.\n",
        "\n",
        "NSP task questioned: Later research (e.g., RoBERTa) showed NSP might not significantly benefit performance.\n",
        "\n",
        "[GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "\n",
        "The GPT (Generative Pre-trained Transformer) series, developed by OpenAI, are autoregressive language models that generate human-like text.\n",
        "\n",
        "GPT models leverage a two-stage process:\n",
        "\n",
        "Pre-training: Learning from large amounts of text data using a language modeling objective.\n",
        "Fine-tuning: Adapting the model to specific tasks with supervised learning and task-specific datasets.\n",
        "\n",
        "Unlike BERT, GPT models use a unidirectional (left-to-right) transformer decoder architecture, making them particularly effective for text generation tasks.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Autoregressive Learning: Predicts the next token given previous tokens, enabling coherent text generation.\n",
        "\n",
        "Transformer Decoder Architecture: Uses self-attention layers but with masked attention to prevent future token leakage.\n",
        "\n",
        "Scalable Architecture: Performance improves with increased parameters, data, and compute (following scaling laws).\n",
        "\n",
        "Few-shot, One-shot, and Zero-shot Learning: GPT models (especially GPT-3) can perform tasks with little or no fine-tuning, using natural language prompts.\n",
        "\n",
        "GPT-4 (2023):\n",
        "Multimodal capabilities (accepts text and image inputs).\n",
        "\n",
        "Improved reasoning, factual accuracy, and alignment with human values.\n",
        "\n",
        "Better at following nuanced instructions and performing more complex reasoning tasks.\n",
        "\n",
        "Often paired with Reinforcement Learning from Human Feedback (RLHF) for alignment and safety.\n",
        "\n",
        "Emergent Abilities: Enhanced problem-solving, coding (via Codex), and domain-specific applications.\n",
        "\n",
        "Training Objective\n",
        "Causal Language Modeling (CLM):\n",
        "Predicts the next token given the previous context (unidirectional).\n",
        "Uses masked self-attention to prevent peeking at future tokens during training.\n",
        "\n",
        "No NSP (Next Sentence Prediction): Unlike BERT, GPT does not require an NSP objective, simplifying training and focusing purely on language generation.\n",
        "\n",
        "Strength:\n",
        "\n",
        "Versatile Generation: Produces coherent, human-like text across a wide range of topics.\n",
        "\n",
        "Minimal Fine-tuning Required: Capable of zero-shot learning based solely on prompt design.\n",
        "\n",
        "Emergent Capabilities: Complex reasoning, arithmetic, and coding abilities emerge with scale.\n",
        "\n",
        "Powerful for Few-shot Tasks: Learns from a handful of examples provided in prompts.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Hallucinations: Generates plausible but incorrect or nonsensical answers.\n"
      ],
      "metadata": {
        "id": "d_xXeLH2pTUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readings for the week\n",
        "\n",
        "[Paper](https://arxiv.org/pdf/2001.08361/1000)\n",
        "\n",
        "Summary:\n",
        "\n",
        "In \"Scaling Laws for Neural Language Models,\" the authors empirically investigate how language model performance, measured by cross-entropy loss, scales with model size, dataset size, and training compute. They discover that loss decreases predictably following a power-law as these factors increase, with trends spanning over seven orders of magnitude. Also, variations in architectural details like network width and depth have minimal impact on performance within a broad range. The study also reveals that larger models are more sample-efficient, achieving better performance with fewer data points. Consequently, for a fixed compute budget, the optimal strategy involves training large models on relatively modest datasets and halting training well before full convergence."
      ],
      "metadata": {
        "id": "KI-ziAo-5x4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Paradigm of Representation learning\n",
        "\n",
        "[JEPA Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/a600f0f740605205133553cb74a1c107-Paper-Conference.pdf)"
      ],
      "metadata": {
        "id": "RaGBvNMaXFzr"
      }
    }
  ]
}