{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO93uLGTWhFxYzukMHF3rMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI_4170_summer/blob/main/Lecture_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 14\n",
        "\n",
        "# Topics for today\n",
        "\n",
        "1. Introduction to Natural Language Processing(NLP)\n",
        "2. Introduction to Language Models\n",
        "3. Text Related Problems\n",
        "4. Attention Mechanism and Transformers"
      ],
      "metadata": {
        "id": "fB5wIGt93JNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Any task involving machines dealing with human language falls under the umbrella of NLP. Tasks such as translating between languages,  text analysis, speech recognition and automatic text generation all are common NLP taks. NLP finds applications in voice assistants like Siri and Alexa, and even search engines such as Google and Bing. ChatGPT is one of the recent additions to NLP applications.\n",
        "\n",
        "Not all NLP tasks involve Machine learning, for example, search engines rely largely on non-ML algorithms to find the most relevant documents or web pages for a given search query.\n",
        "\n",
        "## Key NLP Tasks\n",
        "\n",
        "Tokenization, stemming (Stemming is a technique used to reduce an inflected word down to its word stem), lemmatization (Describes the algorithmic process of identifying an inflected wordâ€™s based on its intended meaning).\n",
        "\n",
        "Part-of-speech tagging, named entity recognition.\n",
        "\n",
        "Syntactic parsing and semantic analysis.\n",
        "\n",
        "## Challenges in NLP\n",
        "\n",
        "Ambiguity in language.\n",
        "\n",
        "Context understanding and sarcasm detection.\n",
        "\n",
        "Multilingual and low-resource language processing."
      ],
      "metadata": {
        "id": "wATsjgao3elW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary\n",
        "\n",
        "Most of the human generated language is either in the form of text or speech. Both come from the language vocabulary. When processing raw data everything is done via vocabulary.\n",
        "\n",
        "\n",
        "1. Text Corpus: the text corpus refers to the set of texts used for the task. For example, if we were building a model to analyze news articles, our text corpus would be the entire set of articles or papers we used to train and evaluate the model.\n",
        "\n",
        "2. Tokenization: A piece of text can be represented as a vector/list of its vocabulary words. This process is known as tokenization, where each individual vocabulary word in a piece of text is a token. Types:\n",
        "\n",
        "    a. Word tokenization. This method breaks text down into individual words. It's the most common approach and is particularly effective for languages with clear word boundaries like English.\n",
        "\n",
        "    b. Character tokenization. Here, the text is segmented into individual characters. This method is beneficial for languages that lack clear word boundaries or for tasks that require a granular analysis, such as spelling correction.\n",
        "\n",
        "    c. Subword tokenization. Striking a balance between word and character tokenization, this method breaks text into units that might be larger than a single character but smaller than a full word. For instance, \"Chatbots\" could be tokenized into \"Chat\" and \"bots\". This approach is especially useful for languages that form meaning by combining smaller units or when dealing with out-of-vocabulary words in NLP tasks.\n",
        "\n",
        "[Source](https://www.datacamp.com/blog/what-is-tokenization)\n",
        "\n",
        "3. Embeddings: Vector representation for words in vocabulary. Examples include Word2vec, Glove etc.The basis for embedding vectors comes from the concept of a target word and its context window. For each word of each tokenized sequence in the text corpus, we treat it as a target, and the words around it as the context window.\n",
        "\n",
        "4. Candidate Sampling: Training an embedding model is equivalent to multiclass classification, where the possible classes include every single vocabulary word. In order to mitigate the full softmax operation, we apply something called candidate sampling. Under candidate sampling, we choose a much smaller fraction of the possible classes (i.e. vocabulary words) for computing the loss."
      ],
      "metadata": {
        "id": "lBp-C5wV3_Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization example\n",
        "import tensorflow as tf\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='OOV') #num_words parameter can be set to vocab size\n",
        "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "new_texts = ['bob ate fruit', 'fred ate pears']\n",
        "print(tokenizer.texts_to_sequences(new_texts))\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GVwOKAAvq7T",
        "outputId": "ebdb98f6-1843-47f9-9a27-0fb8c7748ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1], [7, 2, 6]]\n",
            "{'OOV': 1, 'ate': 2, 'apples': 3, 'bob': 4, 'and': 5, 'pears': 6, 'fred': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Probabilities\n",
        "\n",
        "The goal of any language model is to assign probabilities to words in a given text sequence. This is a multiclass classification problem since each word can be considered a class with the words appearing before it as input.\n",
        " Original sentence = ['She', 'went', 'to', 'buy','a','book']\n",
        "\n",
        " Input =  ['She', 'went', 'to', 'buy','a']\n",
        "\n",
        " Target = ['went', 'to', 'buy','a','book']\n",
        "\n",
        " The language model attempts to predict each word in the target sequence based on its corresponding prefix in the input. In this example we can have the following pairs of input-target sequences. (she,went),((she,went),to) and so on.\n",
        "We can choose to fix our training set sequences to a given maximum length."
      ],
      "metadata": {
        "id": "60fWJX7mM5Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions\n",
        "1. Sampled softmax loss: Instead of doing softmax over all items in the catalog, a small number of negatives are sampled.This approach can be considered an approximation of the initial softmax over the whole item collection\n",
        "\n",
        "[Link](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)\n",
        "\n",
        "2. Noise contrastive estimation (NCE): Covert multiclass classification problem to binary classification problem.\n",
        "\n",
        "[Link](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)\n",
        "\n",
        "[Link](https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html)\n",
        "\n",
        "[Paper](https://arxiv.org/pdf/1809.01812)\n",
        "\n",
        "Other important aspects of working with embeddings: Cosine similarity, K-nearest neighbor algorithm."
      ],
      "metadata": {
        "id": "F0bEkJXaJh6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Language Models\n",
        "\n",
        "We have discussed RNNs, LSTMS, GRUs etc. These are neural network architectures for sequential data.\n",
        "\n",
        "[Project](https://github.com/Uzmamushtaque/Projects-in-Machine-Learning-and-AI/blob/main/SentimentAnalysis_LSTM.ipynb)\n",
        "\n",
        "[Check SOTA](https://paperswithcode.com/task/language-modelling)"
      ],
      "metadata": {
        "id": "Mcf8AwUtItqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Framework\n",
        "\n",
        "One of the popular frameworks in NLP is the sequence to sequence (seq2seq) framework. This framework involves any task that takes in some text and returns some generated text. Dialog systems (e.g. chatbots), text summarization, and machine translation are all seq2seq applications.\n",
        "\n",
        "Training involves the following tasks:\n",
        "\n",
        "1. Input Task: Extract meaningful information from a given input.\n",
        "\n",
        "2. Output Task: Calculate probabilities of words for the output sequence. This is referred to as language modelling where we process the ground truth sequence with the final token sequence.\n",
        "\n",
        "For seq2seq models, it is important to have start-of-sequence (SOS) and end-of-sequence (EOS) tokens. These tokens mark the start and end of a tokenized text sequence."
      ],
      "metadata": {
        "id": "AYFGeAZUu21G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learnt about LSTM model and its variants e.g. BiLSTM. In an ecoder-decoder model there are two components: encoder and decoder. For NLP applications encoder is LSTM/BiLSTM.\n",
        "\n",
        "LSTM final state is important because that is the information passed on to the decoder. In TensorFlow it's represented by the LSTMStateTuple object.\n",
        "\n",
        "\n",
        "When the BiLSTM has multiple layers, the final_states output of tf.keras.layers.Bidirectional is a tuple containing the final forward and backward states for each layer. In order to use BiLSTM final states in an encoder-decoder model, we need to combine the forward and backward states. This is because the decoder portion utilizes a regular LSTM, which only has a forward direction."
      ],
      "metadata": {
        "id": "OyzqDc1MHLHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Decoder Architecture\n",
        "\n",
        "What makes an encoder-decoder model so powerful is that the decoder uses the final state of the encoder as its initial state. This gives the decoder access to the information that the encoder extracted from the input sequence, which is crucial for good sequence to sequence modeling.\n",
        "\n",
        "In the case of multiple layers for the encoder-decoder model, each layer's final state in the encoder acts as the initial state for the corresponding layer in the decoder.\n",
        "\n",
        "[Reading](https://pradeep-dhote9.medium.com/seq2seq-encoder-decoder-lstm-model-1a1c9a43bbac)\n"
      ],
      "metadata": {
        "id": "7lLsFXfKJTbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanisms\n",
        "\n",
        "In general, long-term dependencies cannot be captured by the final state of the encoder only. Sometimes, the decoder needs access to intermediate states of the encoder to completely model long tem dependencies.\n",
        "\n",
        "Attention mechanism lets the decoder decide which encoder outputs are most useful for the decoder at the current time step.Using the decoder's hidden state at the current time step, as well as the encoder outputs, attention will calculate something called a context vector. The context vector encapsulates the most meaningful information from the input sequence for the current decoder time step, and it's used as additional input for the decoder when calculating the time step's output.\n",
        "\n",
        "Attention makes use of trainable weights to calculate the context vector. There are two most popular techniques for calculating the context vextor - in Tensorflow these are BahdanauAttention and LuongAttention. These two attention mechanisms are named after their main inventors, Dzmitry Bahdanau and Minh-Thang Luong, respectively.\n",
        "The main difference between the two mechanisms is how they combine the encoder outputs and current time step hidden state when computing the context vector. The Bahdanau mechanism uses an additive (concatenation-based) method, while the Luong mechanism uses a multiplicative method.\n",
        "\n",
        "[Link](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/1409.0473.pdf)"
      ],
      "metadata": {
        "id": "MZ2UqD9kMvss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rise of Transformers\n",
        "\n",
        "A foundation model is a transformer model that has been trained on supercomputers with billions of records of data and billions of parameters. The model can then perform a wide range of tasks with no further fine-tuning. The scale of foundation models is unique. These fully trained models are often called engines. Only GPT-3, Google BERT, and a handful of transformer engines can qualify as foundation models.\n",
        "\n",
        "[Link](https://arxiv.org/abs/2108.07258)"
      ],
      "metadata": {
        "id": "uViYv-CwtKlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "A transformer is a deep learning architecture developed by Google with its roots in multi-head attention mechanism. It does not include recurrent units and requires less training time. Transformers are the the key to large language models (LLMs).\n",
        "\n",
        "[Paper 2](https://arxiv.org/pdf/1706.03762.pdf)"
      ],
      "metadata": {
        "id": "E2Dgw8c6R_TJ"
      }
    }
  ]
}