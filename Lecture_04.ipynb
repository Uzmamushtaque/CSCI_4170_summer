{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6CpN9ddHyUGrPwis884vQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI_4170_summer/blob/main/Lecture_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Today's Lecture\n",
        "\n",
        "1. Review ensemble learning\n",
        "2. Important Boosting Agorithms\n",
        "3. XGBoost\n",
        "4. Important Research in the field of ensemble learning"
      ],
      "metadata": {
        "id": "lQrwZnZZ9e5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree based Methods are extensively used in various applications across all domains. Today's lecture is aimed at emphasizing the techniques that are proven to give good results in most practical settings."
      ],
      "metadata": {
        "id": "U4gZRQs6N2yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting Overview\n",
        "\n",
        "Boosting is a technique through which multiple weak learners are combined adaptively to make accurate predictions. To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)"
      ],
      "metadata": {
        "id": "gHV09qYC9tdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Boosting Algorithms\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "2. Gradient Tree Boosting\n",
        "\n",
        "3. XGBoost"
      ],
      "metadata": {
        "id": "2KJLPriXKuoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost\n",
        "\n",
        "* Build a model and make predictions.\n",
        "\n",
        "* Assign higher weights to miss-classified points.\n",
        "\n",
        "* Build next model.\n",
        "\n",
        "* Repeat steps 3 and 4.\n",
        "\n",
        "* Make a final model using the weighted average of individual models.\n",
        "\n",
        "[Link1](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-adaboost-algorithm-with-python-implementation/)\n",
        "\n",
        "[Link2](https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/#:~:text=AdaBoost%20algorithm%2C%20short%20for%20Adaptive,assigned%20to%20incorrectly%20classified%20instances.)\n"
      ],
      "metadata": {
        "id": "Bz94A3jNNTfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Gradient boosting is a machine learning ensemble technique that combines the predictions of multiple weak learners, typically decision trees, sequentially. It aims to improve overall predictive performance by optimizing the model’s weights based on the errors of previous iterations, gradually reducing prediction errors and enhancing the model’s accuracy.\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2021/09/gradient-boosting-algorithm-a-complete-guide-for-beginners/)"
      ],
      "metadata": {
        "id": "VauRw-yMNv1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "XGBoost is a popular implementation of gradient boosting. Let’s discuss some features of XGBoost that make it so interesting:\n",
        "\n",
        "**Regularization:**  Regularization helps in preventing overfitting. XGBoost uses an option to incorporate both L1 and L2 regularization.\n",
        "\n",
        "**Handling sparse data:** Pre-processing steps like missing values imputation or one-hot encoding make data sparse. XGBoost incorporates a sparsity-aware split finding algorithm to handle different types of sparsity patterns in the data\n",
        "\n",
        "**Weighted quantile sketch:** Most existing tree based algorithms can find the split points when the data points are of equal weights (using quantile sketch algorithm). However, they are not equipped to handle weighted data. XGBoost has a distributed weighted quantile sketch algorithm to effectively handle weighted data\n",
        "\n",
        "**Block structure for parallel learning:** For faster computing, XGBoost can make use of multiple cores on the CPU. This is possible because of a block structure in its system design. Data is sorted and stored in in-memory units called blocks. Unlike other algorithms, this enables the data layout to be reused by subsequent iterations, instead of computing it again. This feature also serves useful for steps like split finding and column sub-sampling\n",
        "\n",
        "**Cache awareness:** In XGBoost, non-continuous memory access is required to get the gradient statistics by row index. Hence, XGBoost has been designed to make optimal use of hardware. This is done by allocating internal buffers in each thread, where the gradient statistics can be stored\n",
        "\n",
        "**Out-of-core computing:** This feature optimizes the available disk space and maximizes its usage when handling huge datasets that do not fit into memory\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)"
      ],
      "metadata": {
        "id": "CcQ43rpr-zAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recent Research using Ensemble Techniques\n",
        "\n",
        "We will read the following papers in class:\n",
        "\n",
        "[Paper 1](https://www.mdpi.com/1999-4893/17/8/353)\n",
        "\n",
        "Summary: This paper presents a novel approach to improving the accuracy of skin cancer diagnosis by integrating Principal Component Analysis (PCA), AdaBoost, and EfficientNet B0 within a Convolutional Neural Network (CNN) framework. The authors utilize PCA for feature extraction to reduce dataset dimensionality while preserving essential characteristics. AdaBoost, an ensemble classifier technique, is employed to enhance classification accuracy by iteratively training weak classifiers and combining their predictions through weighted voting. EfficientNet B0, known for its lightweight architecture and strong image classification performance, serves as the backbone of the CNN model.The model was trained and validated using the ISIC dataset, a comprehensive collection of annotated dermoscopic images. It achieved an accuracy of 91.00% in distinguishing between malignant and benign skin lesions, demonstrating its potential to significantly enhance the efficiency and accuracy of skin cancer diagnosis and classification. The integration of these advanced AI tools and techniques in skin cancer diagnosis can lead to cost reduction and improved patient outcomes, benefiting both patients and healthcare providers.\n",
        "\n",
        "[Paper 2](https://link.springer.com/article/10.1007/s10479-021-04187-w)\n",
        "\n",
        "Summary: The authors compare six machine learning models, including two recent methods: eXtreme Gradient Boosting (XGBoost) and CatBoost. Their empirical findings indicate that XGBoost outperforms other advanced machine learning models in forecasting gold prices.To enhance interpretability, the study employs Shapley Additive Explanations (SHAP). SHAP helps policymakers understand the predictions of complex machine learning models by examining the importance of various features affecting gold prices. The results demonstrate that combining XGBoost with the SHAP approach significantly improves gold price forecasting performance, providing valuable insights for financial institutions, investors, and related firms in making informed decisions.\n",
        "\n",
        "[Paper 3](https://www.mdpi.com/2078-2489/15/7/394)\n",
        "\n",
        "Summary: The study utilizes ensemble classifiers such as Random Forest, AdaBoost, and XGBoost, with Bayesian optimization employed to fine-tune their hyperparameters for optimal performance. SHAP is applied to interpret the models' predictions, providing insights into the influence of each feature on the outcomes. Experimental evaluations on the Cleveland and Framingham heart disease datasets demonstrate that the optimized XGBoost model outperforms others, achieving specificity and sensitivity values of 0.971 and 0.989 on the Cleveland dataset, and 0.921 and 0.975 on the Framingham dataset, respectively. The integration of ensemble learning, Bayesian optimization, and explainable AI techniques like SHAP offers a promising approach for developing accurate and interpretable models for heart disease prediction, potentially aiding clinicians in making informed decisions.\n",
        "\n",
        "[Paper 4](https://pdf.sciencedirectassets.com/271090/1-s2.0-S0360544223X00255/1-s2.0-S0360544223034552/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECYaCXVzLWVhc3QtMSJIMEYCIQCIifpWz7NB21ZGp%2BWwdgGg6AmOJ7Dr3ujsEiK0qdmQ2AIhAMT9VSsiQOuKN0CUEsdjAqdU0Q7ESzhTAeUEDjiMBv5mKrIFCB8QBRoMMDU5MDAzNTQ2ODY1Igznb6Yp9h04jAm4H3IqjwWs66E02%2BcVrcVfdBCVpMFFk4vIZXxpduehisaTqi%2BZv8FjkTNe3RbCjBIa%2FdFaGdge%2FHcsYlNbFz%2FJA%2FtIBS0R4QqBpWIERuZeae5RWMJIoXla4GkhqiHeMgaZ%2FzBu48y%2BResMeMPOt7CODGPbwgTqpfEFhE9r0y9W4p58xHPxwPZfJEv1Qd0fHEXI33htbH5L0E%2BkcSMcBZweVE%2FioqurWgmfpLX3Ob427SunubO6%2FYHcuxixdrgQm%2FTcMeXoH5%2B53dWEH7Aknc7NooshMgxl50c2fgV3nLXLz%2FBVf%2FPM0u32UCufCMkh%2BshUDmngvpha5%2Bf8iTR2sQIWA5uIRW%2FOkPYpp4fj7p00wASljRk%2ByX8lbtBsrXrzaH8R%2FEqF4m83P8yaPu2BtwNYPchw%2FmpFiwzMjg7qVKpsYIasKccZ3eNCmUZMKfqw05lsV1OTnozpXei%2BdoywXLcNkVBOS%2FEUnbyzLgbJ1c3%2BowjraB4g6FhWJKCG%2B4JAYDuJflSVy7jIWDC846MpL7DL6xqau9ejlYaF%2FRfvvzHAhUeL7u0kapXsp4ApxLGvKi3LHJHjTW9E5TFKljDTmz8CrZLHTSEzo19nTO5XSENg7ALRkxRkU7EY%2BEJienB0GPaO81Ai43JLnT%2FuJrKpFkzL%2B5K3MDdIpbZEcJNwyZOEvdrkMXieISypkbfuE8dHoCYyCrZIXX7Fokt8XddcV4uco6Lw%2FSGnvOrBaBz4hDN%2BH8fqjSfhF8X7xFfpRYiEnlAh9PwfSdmcLOaCsbG8hmEslgeKn1ABFijvd%2FVNi18MUjeaix0RfwXuAJiPTxkeLQIGQjUo5Z%2FBVlju498xAngsyIhT4Ujrob3cZeCg6NYZSZ7yRnzSMIy2m7wGOrABahwcRzv6Mp4B5n%2BE%2FzuOJN1BCNp0MCmJOWnipslvR5NfzxERcgfWjLiif17F5N3kvX23bOmkwo2F8eO8X0Ft9hGbEq%2FLqK1MRcpwI5r4i216YcT9ZKe0MIYEPNJ5frX%2Bx3jlG8Nu0Kt%2BS9AA37nOWwyPsBrgGRWONHbnUhwBiCWb76Eu8eGJFbEQCWw1B5TaaRc01cxGWDFuxRbIW8bf8%2Fjw46wvLj7%2BiZs8qBT7%2Flo%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250114T224659Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRGZ76P74%2F20250114%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e659e22344741cc4d0220ab0e6eebd82eb058deb8a0c7adfd97fcf8e8e0726f3&hash=76a0120d4509f92e0298754e94fbd6cd55abb3b2c349b27c64d5cdcb5dd1e951&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0360544223034552&tid=spdf-a34d511c-282c-4df2-8988-ad35c3f25586&sid=72d3c10b6a45f640094b77675290cbb6943cgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=13135f5454535d045706&rr=90212a4e9ea41b65&cc=us)"
      ],
      "metadata": {
        "id": "fspuS0CY71MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary: The authors compiled a comprehensive dataset encompassing 27 key input features related to catalyst structure, preparation, activation, and FTS operating conditions. They employed feature engineering and selection techniques to identify significant catalyst formulation descriptors and utilized principal component analysis to reduce the dataset's dimensionality. Three machine learning algorithms were tested to predict carbon monoxide (CO) conversion and C5+ selectivity, with the Random Forest model incorporating four principal components achieving the highest accuracy (R² of 0.91 for CO conversion and 0.97 for C5+ selectivity). This framework offers a reliable strategy for designing efficient carbon-supported catalysts in FTS, guiding experimental efforts by pinpointing key descriptors in catalyst formulation and operating conditions to enhance liquid fuel selectivity."
      ],
      "metadata": {
        "id": "sdJpdXb7ADul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research papers\n",
        "\n",
        "Readings for the week:\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/2112.02365.pdf)\n",
        "\n",
        "[Paper 2](https://arxiv.org/pdf/2103.06261.pdf)\n",
        "\n",
        "Reference for research in the field:\n",
        "\n",
        "[Tree Based Models](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)"
      ],
      "metadata": {
        "id": "ER4FBitMTaqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Reference for Research\n",
        "\n",
        "[Link](https://paperswithcode.com/)"
      ],
      "metadata": {
        "id": "LMQ6uFYt8w-l"
      }
    }
  ]
}